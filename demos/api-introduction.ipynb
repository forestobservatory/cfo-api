{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forest Observatory Webinar API demo\n",
    "\n",
    "This notebook walks through the central `cfo` Python API functions and demonstrates some potential applications. This includes searching for data, downloading it locally, rendering map tiles & clipping data using other spatial datasets.\n",
    "\n",
    "This notebook is running in the cfo `conda` environment. We created this simple environment to include a series of useful geospatial/data science packages we use on a regular basis (`gdal`, `otb`, `geopandas`, `sklearn`, etc.). If you have conda [installed](https://docs.conda.io/projects/conda/en/latest/user-guide/install/), you can run the following to run this notebook:\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/forestobservatory/cfo-api.git\n",
    "cd cfo-api\n",
    "conda env update\n",
    "conda activate cfo\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "### What we'll cover\n",
    "\n",
    "The `cfo` package is pretty simple. We designed it to make it easy for users to access and use CFO data. This notebook will address the following topics:\n",
    "\n",
    "- Searching for data\n",
    "- Downloading data\n",
    "- Loading web maps\n",
    "\n",
    "You'll need a CFO account to run everything in this notebook. Sign up at [forestobservatory.com](https://forestobservatory.com).\n",
    "\n",
    "### A note on cloud data storage\n",
    "\n",
    "This notebook review how to download the data using Salo Science's API. But if you want to skip the middle man (erm, the middle alien) you can find the data on our publicly-accessible Google Cloud Storage Bucket ([link here](https://console.cloud.google.com/storage/browser/cfo-public)). If you use `gsutil`, you can access the data via the `gs://cfo-public` bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!!!!!!!!!!!!!!!\n",
    "# change this to a directory on your local machine to determine where downloads will go\n",
    "output_directory = '/home/cba/cfo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# package prep\n",
    "import os\n",
    "import cfo\n",
    "import gdal\n",
    "import ipyleaflet\n",
    "import rasterio\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from ipyleaflet import Map, WMSLayer, LayersControl, basemaps\n",
    "\n",
    "# set plotting style\n",
    "%matplotlib notebook\n",
    "\n",
    "# create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_directory):\n",
    "    os.mkdir(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searching for data\n",
    "\n",
    "Forest Observatory data are organized by `asset_id`, and you'll need to acquire these IDs before you can do anything else. We built the `search` function to facilitate this. It helps to understand the asset ID format:\n",
    "\n",
    "```python\n",
    "{geography}-{category}-{metric}-{year}-{timeOfYear}-{resolution}\n",
    "```\n",
    "\n",
    "We'll explore these in more detail below.\n",
    "\n",
    "When you run a `search` command it returns a `list` of asset IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we'll create an api class instance\n",
    "forest = cfo.api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for all California-wide vegetation datasets\n",
    "ca = forest.search(geography='California', category='Vegetation')\n",
    "\n",
    "# list the results\n",
    "ca.sort()\n",
    "for asset in ca:\n",
    "    print(asset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can search the daily weather datasets we produce, which are deleted from the api after two weeks\n",
    "\n",
    "# list the wind speed assets\n",
    "wind = forest.search(metric='WindSpeed')\n",
    "wind.sort()\n",
    "for asset in wind:\n",
    "    print(asset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the CFO data on the website have been clipped by County, Watershed and Municipality. \n",
    "# you can search those, too. Try it using wildcards\n",
    "\n",
    "# we'll search an ambiguous area\n",
    "sc = forest.search(geography='santacruz*')\n",
    "sc.sort()\n",
    "for asset in sc:\n",
    "    print(asset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cool! But, what if I don't already know what to enter into those fancy category=\"blah blah blah\" fields?\n",
    "\n",
    "We've built some convenience functions to help you out! The search function accepts keywords that match the `asset_id` formatting above. So, `forest.search(geography=str, category=str, metric=str, year=int, timeOfYear=str, resolution=int)`.\n",
    "\n",
    "The convenience functions (under `forest.list_*` can guide some suitable inputs for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the categories of data we generate\n",
    "forest.list_categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the metrics in the Vegetation category\n",
    "forest.list_metrics('Vegetation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and the weather metrics\n",
    "forest.list_metrics('Weather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the different types of geography we slice data by\n",
    "forest.list_geography_types()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the first few county names\n",
    "forest.list_geographies('County')[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which states do we cover?\n",
    "forest.list_geographies('State')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":sunglasses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wanna be a rebel and just get every dang asset we've published?\n",
    "thewholeshebang = forest.search()\n",
    "print(f\"Number of CFO assets: {len(thewholeshebang)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":astonished:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for more help, run\n",
    "forest.search?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gimme the highest resolution data you have!\n",
    "\n",
    "Thanks for asking nicely. We're still in the process of publishing our 3m datasets, and we appreciate your patience as we do so. Here's how you would search for the statewide datasets at 10m and 3m resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca3 = forest.search(geography='california', resolution=3)\n",
    "ca10 = forest.search(geography='california', resolution=10)\n",
    "print(ca3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading data\n",
    "\n",
    "Though `search` returns a list of asset IDs, the `download` function requires string inputs. This is to reduce the risk of downloading a huge list of files by accident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the most recent tree height data from grass valley\n",
    "gv = forest.search(geography=\"GrassValley*\", metric='CanopyHeight', year=2020)\n",
    "print(f\"Number of files found: {len(gv)}\")\n",
    "\n",
    "# iterate over the list to pass the asset id as a string\n",
    "for asset in gv:\n",
    "    filename = os.path.join(output_directory, asset + '.tif')\n",
    "    forest.download(asset, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get all the veg metrics, just iterate over the list\n",
    "gv_time_series = forest.search(geography=\"GrassValley*\", category=\"Vegetation\")\n",
    "for asset in gv_time_series:\n",
    "    filename = os.path.join(output_directory, asset + '.tif')\n",
    "    forest.download(asset, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's happening under the hood?\n",
    "\n",
    "Great segue. Thanks for asking. The `download` function calls another routine, `fetch()` to get a URL to where the file is stored on our Google Cloud Storage Bucket. The fetch function is used to retrieve a series of different asset-specific URLs.\n",
    "\n",
    "Downloads use a \"signed url\", which means you can just directly download the file without needing to authenticate with Google first. We then make a `GET` request using this url to stream the file to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the long and convoluted signed url\n",
    "for asset in gv:\n",
    "    print(forest.fetch(asset, dl=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatively you can just get the direct bucket uri\n",
    "for asset in gv:\n",
    "    print(forest.fetch(asset, bucket=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set multiple flags to return a dictionary of urls\n",
    "for asset in gv:\n",
    "    urls = forest.fetch(asset, dl=True, bucket=True, wms=True, gdal=True)\n",
    "    for key in list(urls.keys()):\n",
    "        print(f\"{key}: {urls[key]}\")\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom data clipping\n",
    "\n",
    "We clipped all of the 2020 10-meter Vegetation data by County, Municipality & Watershed based on feedback from the community that these were the most relevant areas and datasets of interest. You're welcome.\n",
    "\n",
    "But we imagine many people are going to be interested in analyzing custom areas of interest. How do you download data from an AOI vector file?\n",
    "\n",
    "The best way to do this with the API is through `gdal`, referencing the statewide datasets.\n",
    "\n",
    "`gdalwarp` has a really nice driver for handling [cloud datasets](https://gdal.org/user/virtual_file_systems.html) that allows you to clip rasters on Google Cloud Storage. \n",
    "\n",
    "We'll run through an example now to use a `geojson` file to download canopy height data from the perimeter of the CZU fire in the Santa Cruz Mountains. This will use `gdal`'s python bindings, but we'll also show what the command line version would look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this vector is in cfo-api/demos\n",
    "vector = os.path.join(os.getcwd(), \"czu-perimeter.geojson\")\n",
    "\n",
    "# get the 2020 10m canopy height asset id\n",
    "czu_search = forest.search(geography='California', metric='CanopyHeight', year=2020, resolution=10)\n",
    "czu = czu_search[0]\n",
    "\n",
    "# and get the gdal file path reference\n",
    "input_file = forest.fetch(czu, gdal=True)\n",
    "print(f\"We'll download from: {input_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the output raster file\n",
    "output_file = os.path.join(output_directory, f\"{czu}.tif\")\n",
    "\n",
    "# set the gdalwarp options\n",
    "options = gdal.WarpOptions(\n",
    "    creationOptions = [\"COMPRESS=DEFLATE\", \"TILED=YES\", \"BIGTIFF=YES\", \"NUM_THREADS=ALL_CPUS\"],\n",
    "    cutlineDSName = vector,\n",
    "    cropToCutline = True\n",
    ")\n",
    "\n",
    "# and run the command\n",
    "warp = gdal.Warp(output_file, input_file, options=options)\n",
    "warp.FlushCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is akin to running something like this at the command line:\n",
    "\n",
    "```\n",
    "gdalwarp /vsigs/cfo-public/vegetation/California-Vegetation-CanopyHeight-2020-Summer-00010m.tif /home/cba/cfo/California-Vegetation-CanopyHeight-2020-Summer-00010m.tif -cutline /home/cba/src/cfo-api/demos/czu-perimeter.geojson -crop_to_cutline\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and mask the data\n",
    "source = rasterio.open(output_file)\n",
    "height = source.read(1).astype(float)\n",
    "height[height == source.nodata] = np.nan\n",
    "\n",
    "# get the range to show\n",
    "vmin = np.nanpercentile(height, 5)\n",
    "vmax = np.nanpercentile(height, 95)\n",
    "\n",
    "# and plot\n",
    "plt.plot(figsize=(15,15), dpi=100)\n",
    "height_map = plt.imshow(height, vmin=vmin, vmax=vmax, cmap=plt.cm.cividis)\n",
    "plt.title(\"Canopy height inside the\\nburn perimeter of the CZU fire\")\n",
    "colorbar = plt.colorbar(height_map)\n",
    "colorbar.set_label(\"Canopy Height (m)\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest.fetch(\"MendocinoCounty-Vegetation-CanopyHeight-2020-Fall-00010m\", wms=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WMS URLs don't always easily plug and play with different rendering services, but they should work with a little nudging. Here's how to use the above URL to visualize these data with `ipyleaflet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wms = WMSLayer(\n",
    "    url='https://maps.salo.ai/geoserver/cfo/wms?p0=0.0&p2=1.44&p25=18.0&p30=21.599999999999998&p50=36.0&p60=43.199999999999996&p75=54.0&p90=64.8&p98=70.56&p100=72.0',\n",
    "    layers=\"cfo:MendocinoCounty-Vegetation-CanopyHeight-2020-Summer-00010m\",\n",
    "    name=\"Mendocino Canopy Height\",\n",
    "    styles=\"vegetation\",\n",
    "    format=\"image/png8\",\n",
    "    transparent=True,\n",
    "    attribution=\"Forest Observatory Â© <a href=https://salo.ai'>Salo Sciences</a>\",\n",
    ")\n",
    "m = Map(basemap=basemaps.Stamen.Terrain, center=(39.39,-123.33), zoom=10)\n",
    "m.add_layer(wms)\n",
    "control = LayersControl(position='topright')\n",
    "m.add_control(control)\n",
    "m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
